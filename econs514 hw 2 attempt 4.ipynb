{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.387\n",
      "Model:                            OLS   Adj. R-squared:                  0.386\n",
      "Method:                 Least Squares   F-statistic:                     279.2\n",
      "Date:                Mon, 27 Feb 2023   Prob (F-statistic):          6.52e-232\n",
      "Time:                        22:39:02   Log-Likelihood:                -3319.4\n",
      "No. Observations:                2217   AIC:                             6651.\n",
      "Df Residuals:                    2211   BIC:                             6685.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -10.0716      0.253    -39.822      0.000     -10.568      -9.576\n",
      "x1             0.2650      0.043      6.146      0.000       0.180       0.350\n",
      "x2            -0.0343      0.073     -0.472      0.637      -0.177       0.108\n",
      "x3             2.3421      0.125     18.707      0.000       2.097       2.588\n",
      "x4            -0.1243      0.277     -0.448      0.654      -0.668       0.419\n",
      "x5            -0.0886      0.004    -22.014      0.000      -0.097      -0.081\n",
      "==============================================================================\n",
      "Omnibus:                      158.000   Durbin-Watson:                   1.478\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              205.595\n",
      "Skew:                          -0.632   Prob(JB):                     2.27e-45\n",
      "Kurtosis:                       3.792   Cond. No.                         203.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                          IV2SLS Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.224\n",
      "Model:                         IV2SLS   Adj. R-squared:                  0.222\n",
      "Method:                     Two Stage   F-statistic:                     151.3\n",
      "                        Least Squares   Prob (F-statistic):          1.84e-138\n",
      "Date:                Mon, 27 Feb 2023                                         \n",
      "Time:                        22:39:02                                         \n",
      "No. Observations:                2217                                         \n",
      "Df Residuals:                    2211                                         \n",
      "Df Model:                           5                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -9.7475      0.302    -32.269      0.000     -10.340      -9.155\n",
      "x1             0.0712      0.078      0.917      0.359      -0.081       0.223\n",
      "x2             1.0455      0.348      3.009      0.003       0.364       1.727\n",
      "x3             2.2374      0.145     15.471      0.000       1.954       2.521\n",
      "x4             2.6760      0.930      2.878      0.004       0.853       4.499\n",
      "x5            -0.1863      0.031     -6.035      0.000      -0.247      -0.126\n",
      "==============================================================================\n",
      "Omnibus:                       86.419   Durbin-Watson:                   1.400\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              257.658\n",
      "Skew:                          -0.034   Prob(JB):                     1.12e-56\n",
      "Kurtosis:                       4.669   Cond. No.                         203.\n",
      "==============================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.573\n",
      "Model:                            OLS   Adj. R-squared:                  0.571\n",
      "Method:                 Least Squares   F-statistic:                     493.3\n",
      "Date:                Mon, 27 Feb 2023   Prob (F-statistic):               0.00\n",
      "Time:                        22:39:02   Log-Likelihood:                -6984.9\n",
      "No. Observations:                2217   AIC:                         1.398e+04\n",
      "Df Residuals:                    2210   BIC:                         1.402e+04\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.0456      1.406      0.032      0.974      -2.712       2.803\n",
      "x1            -2.9446      0.264    -11.155      0.000      -3.462      -2.427\n",
      "x2             9.9354      0.344     28.898      0.000       9.261      10.610\n",
      "x3            -0.8568      0.681     -1.259      0.208      -2.192       0.478\n",
      "x4            27.3161      1.332     20.505      0.000      24.704      29.929\n",
      "x5            -0.0666      0.056     -1.196      0.232      -0.176       0.043\n",
      "x6             0.0554      0.008      6.967      0.000       0.040       0.071\n",
      "==============================================================================\n",
      "Omnibus:                     1112.714   Durbin-Watson:                   0.951\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8929.792\n",
      "Skew:                           2.228   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.764   Cond. No.                     1.54e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.54e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  price   R-squared:                       0.656\n",
      "Model:                            OLS   Adj. R-squared:                  0.656\n",
      "Method:                 Least Squares   F-statistic:                     844.9\n",
      "Date:                Mon, 27 Feb 2023   Prob (F-statistic):               0.00\n",
      "Time:                        22:39:02   Log-Likelihood:                -567.01\n",
      "No. Observations:                2217   AIC:                             1146.\n",
      "Df Residuals:                    2211   BIC:                             1180.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.7929      0.046     61.188      0.000       2.703       2.882\n",
      "x1            -0.4706      0.049     -9.694      0.000      -0.566      -0.375\n",
      "x2             0.6798      0.019     36.247      0.000       0.643       0.717\n",
      "x3             0.1248      0.063      1.967      0.049       0.000       0.249\n",
      "x4             0.5203      0.035     14.833      0.000       0.452       0.589\n",
      "x5             0.0128      0.002      8.526      0.000       0.010       0.016\n",
      "==============================================================================\n",
      "Omnibus:                      533.211   Durbin-Watson:                   0.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1412.614\n",
      "Skew:                           1.270   Prob(JB):                    1.80e-307\n",
      "Kurtosis:                       5.974   Cond. No.                         150.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "class BLP(object):\n",
    "    '''\n",
    "    Jia Yan\n",
    "    02/21/2023\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, path_data, file_data, path_str_data, file_str_data, ndraws=500, tol_fp=1e-12):\n",
    "        # set parameters\n",
    "        self.ndraws = ndraws\n",
    "        self.tol_fp = tol_fp\n",
    "        \n",
    "        # create data\n",
    "        data = scipy.io.loadmat(os.path.join(path_data, file_data))\n",
    "        model_name = scipy.io.loadmat(os.path.join(path_str_data, file_str_data))['model_name']\n",
    "        v_list = ['outshr', 'const', 'mpd', 'mpg', 'air', 'space', 'hpwt', 'price', 'trend']\n",
    "        self.nmarkets = data['trend'].max() + 1\n",
    "        self.df = data['share']\n",
    "        for item in v_list:\n",
    "            self.df = np.concatenate([self.df, data[item]], axis=1)\n",
    "        self.df = pd.DataFrame(self.df, columns = ['share'] + v_list)\n",
    "        self.df['model_name'] = model_name\n",
    "        self.df['maker'] = self.df['model_name'].transform(lambda x: x[0:2])\n",
    "        \n",
    "        # demand model variables\n",
    "        self.attributes = ['const', 'mpd', 'air', 'space', 'hpwt', 'price']\n",
    "        \n",
    "        # demand model variables with random coefficient\n",
    "        self.attributes_random = ['const', 'mpd', 'air', 'space', 'hpwt']\n",
    "        \n",
    "        # marginal cost variables\n",
    "        self.mc = ['const', 'mpg', 'air', 'space', 'hpwt', 'trend']\n",
    "        self.Xmat = self.df[self.attributes].to_numpy()\n",
    "        self.Mcmat = self.df[self.mc].copy()\n",
    "        self.Mcmat['mpg'] = np.log(self.Mcmat['mpg'])\n",
    "        self.Mcmat['space'] = np.log(self.Mcmat['space'])\n",
    "        self.Mcmat['hpwt'] = np.log(self.Mcmat['hpwt'])\n",
    "        self.Mcmat = self.Mcmat.to_numpy()\n",
    "        \n",
    "        '''\n",
    "        Take standard normal draws for approximating integrals in market share and mark-up computations.\n",
    "        Better to use halton draws\n",
    "        '''\n",
    "        self.draws = np.random.randn(self.nmarkets, self.ndraws, len(self.attributes_random))    \n",
    "        \n",
    "        '''\n",
    "        creat instruments for price: sum of attributes of rival products\n",
    "        '''\n",
    "        z_list = ['const']\n",
    "        self.IV_list = ['const', 'mpd', 'air', 'space', 'hpwt'] # the first part of IV are exogenous regressors\n",
    "        for var in z_list:\n",
    "            name_own = var + \"_\" + \"z\" + \"_\" + \"own\"\n",
    "            self.IV_list.append(name_own)\n",
    "            name_rival = var + \"_\" + \"z\" + \"_\" + \"rival\"\n",
    "            self.IV_list.append(name_rival)\n",
    "            \n",
    "            self.df[name_own] = self.df.groupby(['trend', 'maker'])[var].transform(lambda x: x.sum())\n",
    "            self.df[name_own] = self.df[name_own] - self.df[var]\n",
    "            \n",
    "            self.df['junk']= self.df.groupby(['trend'])[var].transform(lambda x: x.sum()) - self.df[var]\n",
    "            self.df[name_rival] = self.df['junk'] - self.df[name_own]\n",
    "        \n",
    "        self.Zmat = self.df[self.IV_list].to_numpy()\n",
    "        self.weight_mat = np.linalg.inv(np.matmul(np.transpose(self.Zmat), self.Zmat)) # weighting matrix in GMM estimation\n",
    "        \n",
    "        # creat projection matrix of instruments for future computations \n",
    "        pz = np.matmul(self.Zmat, self.weight_mat)\n",
    "        pz = np.matmul(pz, np.transpose(self.Zmat))\n",
    "        self.project_mat = np.matmul(np.transpose(self.Xmat), pz)\n",
    "        self.project_mat = np.matmul(self.project_mat, self.Xmat)\n",
    "        self.project_mat = np.linalg.inv(self.project_mat)\n",
    "        self.project_mat = np.matmul(self.project_mat, np.transpose(self.Xmat))\n",
    "        self.project_mat = np.matmul(self.project_mat, pz)\n",
    "        \n",
    "        # finally, dependent variable in demand models without random coefficients\n",
    "        self.y_fixed = np.log(self.df['share']/self.df['outshr'])\n",
    "        \n",
    "    def ols(self):\n",
    "        '''\n",
    "        replicate the first column of table 3\n",
    "        '''\n",
    "        y = np.log(self.df['share']/self.df['outshr'])\n",
    "        #b = np.matmul(np.transpose(self.Xmat), self.Xmat)\n",
    "        #b = np.linalg.inv(b)\n",
    "        #b = np.matmul(b, np.transpose(self.Xmat))\n",
    "        #return np.matmul(b, self.y_fixed)\n",
    "        return sm.OLS(self.y_fixed, self.Xmat).fit()\n",
    "        \n",
    "    def iv(self):\n",
    "        '''\n",
    "        replicate the second column of table 3\n",
    "        '''\n",
    "        #return np.matmul(self.project_mat, self.y_fixed)\n",
    "        return IV2SLS(self.y_fixed, self.Xmat, self.Zmat).fit()\n",
    "    \n",
    "    def hedonic_price(self):\n",
    "        '''replicate the third column of table 3'''\n",
    "        y = np.log(self.df['price'])\n",
    "        return sm.OLS(y, self.Mcmat).fit()\n",
    "    \n",
    "    def market_share(self, mid, delta, xv):\n",
    "        draws = self.draws[mid]\n",
    "        s = np.zeros(len(delta))\n",
    "        for r in range(self.ndraws):\n",
    "            w = draws[r]\n",
    "            v = np.exp(delta + (w * xv).sum(axis=1))\n",
    "            s = s + (v / (1 + np.sum(v)))\n",
    "        return (1/self.ndraws) * s \n",
    "    \n",
    "    def fixed_point(self, pack):\n",
    "        mid = pack['mid']\n",
    "        df = pack['df']\n",
    "        sigmas = pack['sigmas']\n",
    "        s0 = df['share'].to_numpy()\n",
    "        xv = sigmas * df[self.attributes_random].to_numpy()\n",
    "        check = 1.0\n",
    "        delta_ini = np.zeros(len(s0))\n",
    "        while check > self.tol_fp:\n",
    "            \n",
    "            delta_new = delta_ini + (np.log(s0) - np.log(self.market_share(mid, delta_ini, xv)))\n",
    "            check = np.max(abs(delta_new - delta_ini))\n",
    "            delta_ini = delta_new\n",
    "        return delta_new\n",
    "        \n",
    "    def mean_utility(self,sigmas):\n",
    "        \"\"\"\n",
    "        sigmas: an 1_D array with the shape (len(self.attributes_random), ), which contains\n",
    "        the standard errors of random coefficients\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        v_list = ['share'] + self.attributes_random\n",
    "        \n",
    "        '''\n",
    "        # step 1: solve mean utility (delta_j) from the fixed-point iteration\n",
    "        '''\n",
    "        df_list = [{'mid': int(mid), 'df': d[v_list], 'sigmas': sigmas} for mid, d in df.groupby(['trend'])]\n",
    "        delta_j = tuple(map(self.fixed_point, df_list))\n",
    "        delta_j = np.concatenate(delta_j, axis=0) # an array with the shape(2217,)\n",
    "        \n",
    "        '''\n",
    "        step 2: uncover mean part of coefficients (beta_bar) from delta_j, which is equivalent to \n",
    "        running an IV estimation using delta_j as the dependent variable\n",
    "        '''\n",
    "        beta_bar = np.matmul(self.project_mat, delta_j) \n",
    "        \n",
    "        '''\n",
    "        step 3: uncover ommited product attributes (xi_j) from delta_j and beta_bar\n",
    "        '''\n",
    "        xi_j = delta_j - np.matmul(self.Xmat, beta_bar)\n",
    "        \n",
    "        return {'beta_bar': beta_bar, 'xi_j': xi_j} \n",
    "    \n",
    "    def GMM_obj(self, sigmas):\n",
    "        \n",
    "        '''\n",
    "        step 4: interact xi_j with instruments,which include exogenous regressors (veihicles' own\n",
    "        exogenous attributes) and instruments for price (sum of attributes of competing products)\n",
    "        '''\n",
    "        xi_j = self.mean_utility(sigmas)['xi_j']\n",
    "        moments = np.matmul(np.transpose(self.Zmat), xi_j) # an array with the shape (m, ), where m is the number of IVs\n",
    "        \n",
    "        '''\n",
    "        step 5: compute the GMM objective function\n",
    "        '''\n",
    "        f = np.matmul(moments, self.weight_mat)\n",
    "        f = (1/len(self.df)) * np.matmul(f, moments)\n",
    "        return f\n",
    "    \n",
    "    def optimization(self, objfun, para):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        objfun : a user defined objective function of para\n",
    "            \n",
    "        para : a 1-D array with the shape (k,), where k is the number of parameters.\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            A dictionary containing estimation results      \n",
    "        '''\n",
    "        v = opt.minimize(objfun, x0=para, jac=None, method='BFGS', \n",
    "                          options={'maxiter': 1000, 'disp': True})  \n",
    "        return {'obj':v.fun, \"Coefficients\": v.x}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #blp = BLP(\"/kaggle/input/blp-data/\", \"BLP_data.mat\", \"/kaggle/input/blp-str-data/\", \"BLP_data_str.mat\")\n",
    "    blp = BLP(\"C:/Users/otten/Desktop/econs514/\",\"BLP_data.mat\",\"C:/Users/otten/Desktop/econs514/\",\"BLP_data_str.mat\")\n",
    "\n",
    "    pini = np.ones(len(blp.attributes_random)) * 0.2\n",
    "    x = blp.GMM_obj(pini)\n",
    "    beta_ols = blp.ols()\n",
    "    beta_iv = blp.iv()\n",
    "    beta_hedonic= blp.hedonic_price()\n",
    "    print(beta_ols.summary())\n",
    "    print(beta_iv.summary())\n",
    "    print(sm.OLS(blp.df['price'], blp.Zmat).fit().summary()) # first-stage regression in IV estimation\n",
    "    print(beta_hedonic.summary())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'N' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-26c2b084a1e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Define initial parameter values and other model parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'N' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define initial parameter values and other model parameters\n",
    "sigma = 1.5\n",
    "mu = np.zeros((N, 1))\n",
    "tolerance = 1e-6\n",
    "max_iter = 500\n",
    "delta = 1.0\n",
    "beta = 0.8\n",
    "xi = np.zeros((N, J))\n",
    "\n",
    "# Define the contraction mapping function\n",
    "def contraction_mapping(theta, delta, beta, xi, mu, sigma, P):\n",
    "    \"\"\"\n",
    "    Calculates the product market constants using the contraction mapping approach.\n",
    "    \"\"\"\n",
    "    error = 1\n",
    "    i = 0\n",
    "    while error > tolerance and i < max_iter:\n",
    "        # Calculate the market shares using the current xi values\n",
    "        exp_delta = np.exp(delta / sigma)\n",
    "        s = np.sum(exp_delta / (1 + np.sum(exp_delta, axis=1, keepdims=True) - exp_delta), axis=1, keepdims=True)\n",
    "        \n",
    "        # Calculate the instruments for the moment conditions\n",
    "        Z = np.hstack((np.ones((N, 1)), X, s, s*X))\n",
    "        \n",
    "        # Calculate the moments\n",
    "        g = np.mean(Z*(P - np.exp(mu + xi)), axis=0)\n",
    "        \n",
    "        # Calculate the weighting matrix for GMM\n",
    "        W = np.eye(g.shape[0])\n",
    "        \n",
    "        # Calculate the update for xi using the GMM estimator\n",
    "        xi_new = xi + np.linalg.inv(Z.T @ W @ Z) @ Z.T @ W @ (P - np.exp(mu + xi) - Z @ g.T).T\n",
    "        \n",
    "        # Calculate the maximum absolute difference between the old and new xi values\n",
    "        error = np.max(np.abs(xi_new - xi))\n",
    "        xi = beta * xi_new + (1 - beta) * xi\n",
    "        i += 1\n",
    "    \n",
    "    # Calculate the average price for each product and market\n",
    "    avg_p = np.mean(np.exp(mu + xi), axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate the market-level constants\n",
    "    c = np.log(np.sum(np.exp(mu + xi) / P, axis=0, keepdims=True)) - np.log(J)\n",
    "    \n",
    "    return c, xi, avg_p\n",
    "\n",
    "# Define the objective function\n",
    "def objective(theta, delta, beta, xi, mu, sigma, P):\n",
    "    \"\"\"\n",
    "    Calculates the GMM objective function for a given set of parameters.\n",
    "    \"\"\"\n",
    "    c, xi, avg_p = contraction_mapping(theta, delta, beta, xi, mu, sigma, P)\n",
    "    exp_delta = np.exp(delta / sigma)\n",
    "    s = np.sum(exp_delta / (1 + np.sum(exp_delta, axis=1, keepdims=True) - exp_delta), axis=1, keepdims=True)\n",
    "    Z = np.hstack((np.ones((N, 1)), X, s, s*X))\n",
    "    error = P - np.exp(c + np.matmul(Z, theta) + xi)\n",
    "    obj = np.mean(error * Z, axis=0)\n",
    "    return obj\n",
    "\n",
    "# Define the main optimization function\n",
    "def optimize(P, X, delta, beta, sigma):\n",
    "    \"\"\"\n",
    "    Optimizes the GMM objective function using the BLP (1995) approach.\n",
    "    \"\"\"\n",
    "    # Define the initial parameter estimates and bounds\n",
    "    theta0 = np.zeros((X.shape[1], 1))\n",
    "    bounds = [(None, None)] * theta0.shape[0]\n",
    "    \n",
    "    # Estimate the demand parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
